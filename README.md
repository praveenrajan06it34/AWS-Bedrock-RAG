# AWS-Bedrock-RAG

Deep learning is a subset of machine learning that uses neural networks with many layers (hence "deep") to model complex patterns in data. It's a powerful approach used for a wide range of applications, from image and speech recognition to natural language processing and autonomous driving.

Large Language Models (LLMs) are a type of artificial intelligence model designed to understand and generate human language. They are built on deep learning architectures, particularly transformer networks, and are trained on vast amounts of text data. This enables them to perform a wide range of natural language processing (NLP) tasks such as text generation, translation, summarization, and more.

Retrieval Augmented Generation (RAG) is a hybrid approach combining information retrieval with generative models to create more accurate and contextually relevant responses. This method enhances the capabilities of large language models (LLMs) by integrating external knowledge retrieved from a database or document store into the response generation process. 

Key Concepts of RAG:

Retrieval:

Embedding Queries and Documents: Queries and documents are converted into vector representations using an embedding model.

Similarity Search: The system retrieves documents from a database based on their similarity to the query embedding.

Augmentation:

Combining Information: The retrieved documents are combined with the query to form an augmented input.

Augmentation in the context of LangChain refers to the process of enriching or enhancing the initial input or query with additional relevant information before further processing. This is crucial for improving the quality and accuracy of the response generated by the language model.

Generation:

LLM Response: The augmented input is fed into an LLM, which generates a response that is informed by the retrieved documents.

How RAG Works:

Query Embedding:

The user's query is embedded using an embedding model, converting it into a high-dimensional vector.

Document Retrieval:

The query embedding is used to search for similar documents in a vector database, retrieving the most relevant ones.

Augmented Input:

The retrieved documents are concatenated with the original query to form an augmented input.

Response Generation:

The augmented input is passed to an LLM, which generates a response based on the combined information.

Vectors are high-dimensional numerical representations of text, which capture the semantic meaning of the input queries and documents. 

Vectors in RAG

Vector Embeddings:

Definition: A vector embedding is a numerical representation of text in a high-dimensional space.

Purpose: To encode the semantic meaning of text, allowing for comparison and similarity measurement.

Generation: Created using embedding models such as BERT, Sentence-BERT, or specialized models like amazon.titan-embed-text-v2:0.

Vector Database:

Purpose: To store and manage vector embeddings of documents for efficient retrieval.

Functionality: Allows for similarity searches to find documents that are semantically similar to a given query vector.

Examples: Chroma, Pinecone, FAISS.

 

Architecture

Open POC.jpg
POC.jpg
 

The system architecture for Retrieval Augmented Generation (RAG) using AWS Bedrock, integrated with Streamlit for the user interface. Here's a breakdown of each component and their interactions:

Components and Workflow:

User Interface (Streamlit):

The user interacts with the system via a Streamlit app, which provides an easy-to-use interface for inputting queries and viewing results.

Streamlit is an open-source Python library that makes it easy to create and share custom web apps for machine learning and data science. It allows to build interactive and aesthetically pleasing web applications with minimal code, making it an excellent tool for data scientists and analysts who want to quickly prototype and share their models and findings.

LangChain:

Streamlit forwards the user input to LangChain, a framework for building applications with language models. LangChain serves as the central orchestrator in this architecture.

LangChain is a powerful framework designed to simplify the process of building applications that use large language models (LLMs). It provides the tools and abstractions needed to connect and orchestrate different components involved in natural language processing tasks.

AWS Bedrock Titan LLM:

LangChain sends queries to AWS Bedrock's Titan Large Language Model (LLM) for generating responses.

Bedrock is an Amazon Web Services platform for building, deploying, and scaling AI models.

Search & Retrieval (Chroma and amazon.titan-embed-text-v2:0):

LangChain also interacts with the search and retrieval components to find relevant information.

amazon.titan-embed-text-v2:0: This model is used for embedding text data, converting text into vector representations.

Chroma: A vector database that stores these embeddings and enables efficient retrieval of relevant documents based on vector similarity.

Source Folder:

This component stores the source data that needs to be indexed and retrieved.

When new data is added to the source folder, it is uploaded, and the vectors are regenerated using the embedding model. These vectors are then stored in Chroma.

Detailed Process:

User Interaction:

The user inputs a query through the Streamlit interface.

Query Processing:

The query is sent to LangChain, which processes it and decides whether to directly generate a response using the Titan LLM or to first retrieve relevant information.

Search & Retrieval:

If retrieval is needed, LangChain sends the query to the amazon.titan-embed-text-v2:0 model to get its embedding.

The embedding is used to search Chroma for relevant documents.

Response Generation:

Retrieved documents are used to augment the query context, which is then sent to the Titan LLM.

The Titan LLM generates a response based on the augmented query.

Displaying Results:

The generated response is sent back to the Streamlit app, which displays it to the user.

Implementation Considerations:

Streamlit Setup: Ensure the Streamlit app is properly configured to handle user inputs and display responses dynamically.

LangChain Configuration: Set up LangChain to handle the orchestration between the different components, managing the flow of data and responses.

AWS Bedrock and Chroma: Ensure that the AWS Bedrock Titan LLM and Chroma are properly integrated with LangChain for seamless query processing and retrieval.

Data Management: Implement mechanisms for adding new data to the source folder and regenerating embeddings to keep the vector database up to date.

This architecture leverages the strengths of each component to create a robust and interactive RAG system, making it easier for users to get precise and relevant information efficiently.
